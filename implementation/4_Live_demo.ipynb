{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "4_Live_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqsz28EtxFB-",
        "colab_type": "text"
      },
      "source": [
        "# Autonomous driving with voice command  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ly5m-Bp3HOF",
        "colab_type": "text"
      },
      "source": [
        "## Load the trained model\n",
        "\n",
        "Upload the model into this notebook's directory by using the Jupyter Lab upload tool. Once that's finished there should be a file named best_model.pth in this notebook's directory \n",
        "\n",
        "\n",
        "Execute the code below to initialize the PyTorch model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p7DIAl0xFCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "model = torchvision.models.alexnet(pretrained=False)\n",
        "model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 4)\n",
        "\n",
        "# Alexnet was introduced in the paper ImageNet Classification with Deep Convolutional Neural Networks \n",
        "# and was the first very successful CNN on the ImageNet dataset. \n",
        "# When we print the model architecture, we see the model output comes from the 6th layer of the classifier\n",
        "# For more information, please see https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZjFOuDTxFCF",
        "colab_type": "text"
      },
      "source": [
        "Next, load the trained weights from the ``best_model.pth`` file that you uploaded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzrkDRk7xFCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load('best_model.pth'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFOLTmloxFCK",
        "colab_type": "text"
      },
      "source": [
        "Currently, the model weights are located on the CPU memory, so you should execute the code below to transfer the model to the GPU device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xga454BBxFCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda')\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fONmuiTzxFCP",
        "colab_type": "text"
      },
      "source": [
        "## Create the preprocessing function\n",
        "\n",
        "You have now loaded our model, but there's a slight issue.  The format that we trained our model doesnt *exactly* match the format of the camera.  To do that, \n",
        "You need to do some *preprocessing*.  This involves the following steps: \n",
        "\n",
        "1. Convert from BGR to RGB\n",
        "2. Convert from HWC layout to CHW layout\n",
        "3. Normalize using same parameters as we did during training (our camera provides values in [0, 255] range and training loaded images in [0, 1] range so we need to scale by 255.0\n",
        "4. Transfer the data from CPU memory to GPU memory\n",
        "5. Add a batch dimension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7aUJo6jxFCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
        "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
        "\n",
        "def preprocess(camera_value):\n",
        "    global device, normalize\n",
        "    x = camera_value\n",
        "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
        "    x = x.transpose((2, 0, 1))\n",
        "    x = torch.from_numpy(x).float()\n",
        "    x = normalize(x)\n",
        "    x = x.to(device)\n",
        "    x = x[None, ...]\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0j8F1RSxFCT",
        "colab_type": "code",
        "outputId": "2c0938b2-ffed-42c4-a5ad-aa00199339ec",
        "colab": {
          "referenced_widgets": [
            "0bc947e218d943a3b4e1a5d938e691b1"
          ]
        }
      },
      "source": [
        "import traitlets\n",
        "from IPython.display import display\n",
        "import ipywidgets.widgets as widgets\n",
        "from jetbot import Camera, bgr8_to_jpeg\n",
        "\n",
        "camera = Camera.instance(width=224, height=224)\n",
        "image = widgets.Image(format='jpeg', width=224, height=224)\n",
        "\n",
        "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
        "\n",
        "display(widgets.HBox([image]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bc947e218d943a3b4e1a5d938e691b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00Câ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTp1KVqKxFCX",
        "colab_type": "text"
      },
      "source": [
        "You'll also create your robot instance which you'll need to drive the motors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EgGdMrHxFCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from jetbot import Robot\n",
        "\n",
        "robot = Robot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbGSraDr3RF6",
        "colab_type": "text"
      },
      "source": [
        "## Autonomous driving logic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHiPmRzNxFCe",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll create a function that will get called whenever the camera's value changes.  This function will do the following steps\n",
        "\n",
        "1. Pre-process the camera image\n",
        "2. Execute the neural network\n",
        "3. While the neural network output indicates we're blocked, we'll turn left, otherwise we go forward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WO4S2xa7SV4",
        "colab_type": "text"
      },
      "source": [
        "The following chart shows the logic of robot movement given the probabilities predicted by the models. and This logic refers to the example provided by [Dmitri Villevald](https://www.hackster.io/dvillevald/transfer-learning-with-nvidia-jetbot-fun-with-cones-adf531)\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://ifh.cc/g/onOTpZ.png' width='800'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "uDjEIB_VxFCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "# Simple PD controller (Kp - proportional term, Kd - derivative term)\n",
        "Kp = 0.18\n",
        "Kd = 0.05\n",
        "\n",
        "frwd_value = 0.3                      # Default value to drive forward (0 = no action, 1 = full motor capacity)\n",
        "rot_value_when_exploring = 0.3        # Default value to spin to the right when robot is in exploration mode (0 = no action, 1 = full motor capacity)\n",
        "min_prob_free_to_drive_frwd = 0.25    # Min probability prob_free for robot to drive forward \n",
        "max_n_frames_stuck = 20               # Limit on the number of frames the robot is stuck for. Once this limit is reached, robot goes into exploration mode (makes large right turn)\n",
        "frame_counter = 0                     # Frame counter \n",
        "n_frames_stuck = 0                    # Initialize counter of the number of successive frames the robot is stuck for\n",
        "exploration = False                   # Initialize binary variable which determines if robot is in exploration mode (when True.) Used to mark the related frames with red background  \n",
        "recent_detections = []                # Initialize the array to store the last frame data\n",
        "\n",
        "def update(change):\n",
        "    global robot, frame_counter, n_frames_stuck, exploration\n",
        "    x = change['new'] \n",
        "    x = preprocess(x)\n",
        "    y = model(x)\n",
        "    \n",
        "    # apply the `softmax` function to normalize the output vector so it sums to 1 (which makes it a probability distribution)\n",
        "    y = F.softmax(y, dim=1)\n",
        "    \n",
        "    y = y.flatten()\n",
        "   \n",
        "    # extract probabilities of blocked, free, left and right\n",
        "    prob_blocked = float(y[0])\n",
        "    prob_free = float(y[1])\n",
        "    prob_left = float(y[2])\n",
        "    prob_right = float(y[3])\n",
        " \n",
        "    # update list of recent detections\n",
        "    while (len(recent_detections) >= 2):\n",
        "        recent_detections.pop(0)\n",
        "    recent_detections.append([prob_free,prob_left,prob_right,prob_blocked])\n",
        "    \n",
        "    # check if robot got stuck and update n_frames_stuck counter\n",
        "    if prob_free < min_prob_free_to_drive_frwd:  # min_prob_free_to_drive_frwd = 0.25\n",
        "        n_frames_stuck = n_frames_stuck + 1 \n",
        "    else:\n",
        "        n_frames_stuck = 0\n",
        "        \n",
        "    # calculate errors at times t (current) and t-1 (prev)    \n",
        "    # error(t) and error(t-1): prob_left-prob_right   \n",
        "    if len(recent_detections) == 2:\n",
        "        current_probs = recent_detections[1]\n",
        "        prev_probs = recent_detections[0]\n",
        "    else:\n",
        "        current_probs = [prob_free,prob_left,prob_right,prob_blocked]\n",
        "        prev_probs = current_probs\n",
        "                \n",
        "    # error = prob_left-prob_right        \n",
        "    current_error = current_probs[1] - current_probs[2]\n",
        "    prev_error    = prev_probs[1] - prev_probs[2]\n",
        "\n",
        "    # increment frame counter \n",
        "    frame_counter = frame_counter + 1\n",
        "    \n",
        "    # define functions which deterine (and return) robot actions\n",
        "    def forward(value):\n",
        "        robot.forward(value)\n",
        "        return (\"FWRD\",round(value,2))\n",
        "\n",
        "    def left(value):\n",
        "        robot.left(value)\n",
        "        return (\"LEFT\",round(value,2))\n",
        "\n",
        "    def right(value):\n",
        "        robot.right(value)\n",
        "        return (\"RGHT\",round(value,2))\n",
        "    \n",
        "    action = \"\"\n",
        "  \n",
        "    # estimate rotational value to turn left (if negative) or right (if positive)\n",
        "    # 0 = no action, 1 = full motor capacity)\n",
        "    rot_value = - Kp * current_error - Kd * (current_error - prev_error)\n",
        "    \n",
        "    # store propotional and differential controller components for frame-by-frame analysis\n",
        "    p_component = - Kp * current_error\n",
        "    d_component = - Kd * (current_error - prev_error)\n",
        "    \n",
        "    # initalize binary flag showinf if robot rotates \n",
        "    robot_rotates = False\n",
        "    \n",
        "    # action logic\n",
        "    # moving forward if there is no obstacles\n",
        "    if prob_free > min_prob_free_to_drive_frwd:\n",
        "        action = forward(frwd_value)\n",
        "\n",
        "    # turn left or right if robot is not blocked for a long time\n",
        "    elif n_frames_stuck < max_n_frames_stuck:\n",
        "        robot_rotates = True\n",
        "        if rot_value < 0.0:\n",
        "            action = left(-rot_value)\n",
        "        else:\n",
        "            action = right(rot_value)\n",
        "\n",
        "    # activate exploration mode - robot turns right by a large (45-90 degree) angle if it failed to move forward for [max_n_frames_stuck] recent frames\n",
        "    else:\n",
        "        exploration = True\n",
        "        robot_rotates = True\n",
        "        action = right(rot_value_when_exploring)\n",
        "        time.sleep(0.5)\n",
        "        n_frames_stuck = 0\n",
        "    \n",
        "    time.sleep(0.001)\n",
        "\n",
        "\n",
        "    # append frame's telemetry and robot action to the stored data \n",
        "    if not robot_rotates:\n",
        "        rot_value = 0.\n",
        "        p_component = 0.\n",
        "        d_component = 0.\n",
        "    if robot_rotates and exploration:\n",
        "        rot_value = rot_value_when_exploring\n",
        "        p_component = 0.\n",
        "        d_component = 0.\n",
        "    \n",
        "    # reset variables\n",
        "    exploration = False\n",
        "    robot_rotates = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3066CA-9-wpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "update({'new': camera.value})  \n",
        "robot.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFxAaJHDxFCk",
        "colab_type": "text"
      },
      "source": [
        "You've created our neural network execution function, but now you need to attach it to the camera for processing. \n",
        "\n",
        "You accomplish that with the observe function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE5xmkNnxFCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "camera.observe(update, names='value')  # this attaches the 'update' function to the 'value' traitlet of our camera"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbyncuRyxFCo",
        "colab_type": "text"
      },
      "source": [
        "Great! If your robot is plugged in it should now be generating new commands with each new camera frame.  Perhaps start by placing your robot on the ground and seeing what it does when it reaches the cones.\n",
        "\n",
        "To stop this behavior, unattach this callback by executing the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3uI-UlBBxFCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "camera.unobserve(update, names='value')\n",
        "update({'new': camera.value}\n",
        "robot.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR1j_YX6xFC-",
        "colab_type": "text"
      },
      "source": [
        "(Optional) Perhaps you want the robot to run without streaming video to the browser.  You can unlink the camera as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvn6iRmMxFC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "camera_link.unlink()  # don't stream to browser (will still run camera)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxZN5rtDxFDA",
        "colab_type": "text"
      },
      "source": [
        "To continue streaming, call the following."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqVIsEGkxFDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "camera_link.link()  # stream to browser (wont run camera)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huIFp_j6326c",
        "colab_type": "text"
      },
      "source": [
        "## Autonomous driving with human intervention\n",
        "We not only let JetBot drive automatically through image processing, but also allow us to control it during autonomous driving through voice commands. \n",
        "\n",
        "If your robot is well trained, it will be easy for your robot to find way. However, your robot may encounter an unlearned situation while finding its way. For example, your robot may be facing a traffic light, at risk of bumping into another robot, or at a crossroads. In that case, you have to send the command to the robot to solve the situation.\n",
        "\n",
        "\n",
        "Therefore, we presented the following simple scenario.\n",
        "\n",
        "<img src='https://ifh.cc/g/vCeRTQ.png' width='500'>\n",
        "\n",
        "\n",
        ">Example\n",
        "\n",
        "><img src='https://ifh.cc/g/fmN9ru.gif' width='800'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tywblzz79SiX",
        "colab_type": "text"
      },
      "source": [
        "To achieve these, we made the same logic as follows:\n",
        "\n",
        "<img src='https://ifh.cc/g/WkyNF3.png' width='750'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV8isquZ49EQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import robotdecoder as snowboydecoder\n",
        "import sys\n",
        "import signal\n",
        "from jetbot import Robot\n",
        "import time\n",
        "\n",
        "# Demo code for listening to four hotwords at the same time\n",
        "\n",
        "robot = Robot()\n",
        "interrupted = False\n",
        "\n",
        "def signal_handler(signal, frame):\n",
        "    global interrupted\n",
        "    interrupted = True\n",
        "\n",
        "\n",
        "def interrupt_callback():\n",
        "    global interrupted\n",
        "    return interrupted\n",
        "\n",
        "# To run the model in terminal, you need to enter the following command\n",
        "# python3 robotmove.py forward.pmdl stop.pmdl right.pmdl left.pmdl\n",
        "# The sys.argv stores: [robotmove.py, forward.pmdl, stop.pmdl, right.pmdl, left.pmdl]\n",
        "\n",
        "if len(sys.argv) != 5:\n",
        "    print(\"Error: need to specify 4 model names\")\n",
        "    sys.exit(-1)\n",
        "\n",
        "\n",
        "models = sys.argv[1:]\n",
        "# models = [forward.pmdl, stop.pmdl, right.pmdl, left.pmdl]\n",
        "\n",
        "# initialize\n",
        "update({'new': camera.value})  \n",
        "robot.stop()\n",
        "# You need to set the logic for your command\n",
        "def wakeup():\n",
        "    camera.observe(update, names='value')\n",
        "\n",
        "def stop():\n",
        "    camera.unobserve(update, names='value')\n",
        "    update({'new': camera.value}\n",
        "    robot.stop()\n",
        "\n",
        "def left():\n",
        "    # Turn left for 1 second\n",
        "    robot.left(0.3)\n",
        "    time.sleep(1)\n",
        "    robot.stop()\n",
        "\n",
        "    # Start autonomous driving\n",
        "    camera.observe(update, names='value')\n",
        "\n",
        "def right():\n",
        "    # Turn right for 1 second\n",
        "    robot.right(0.3)\n",
        "    time.sleep(1)\n",
        "    robot.stop()\n",
        "\n",
        "   # Start autonomous driving\n",
        "    camera.observe(update, names='value')\n",
        "# capture SIGINT signal, e.g., Ctrl+\n",
        "\n",
        "signal.signal(signal.SIGINT, signal_handler)\n",
        "sensitivity = [0.5]*len(models)\n",
        "detector = snowboydecoder.HotwordDetector(models, sensitivity=sensitivity)\n",
        "# In order to work properly, you must match the input and your logic sequence in the command.\n",
        "callbacks = [lambda: wakeup(),\n",
        "             lambda: stop(),\n",
        "             lambda: left(),\n",
        "             lambda: right()]\n",
        "\n",
        "print('Listening... Press Ctrl+C to exit')\n",
        "\n",
        "# main loop\n",
        "# make sure you have the same numbers of callbacks and models\n",
        "detector.start(detected_callback=callbacks,\n",
        "               interrupt_check=interrupt_callback,\n",
        "               sleep_time=0.03)\n",
        "\n",
        "detector.terminate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ufc2LHZHN_i",
        "colab_type": "text"
      },
      "source": [
        "You can run it from the terminal using the following command. (You should save the notebook file as ```robotmove.py```)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1t_on54HTQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " $ python3 robotmove.py wakeup.pmdl stop.pmdl right.pmdl left.pmdl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zil4DLCH_xoy",
        "colab_type": "text"
      },
      "source": [
        "You can also run it on Jupyter notebooks. To do this, you need to:\n",
        "\n",
        "* Move pmdl files in the same directory as the running Jupyter file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vIW1bA__4Ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import robotdecoder as snowboydecoder\n",
        "import sys\n",
        "import signal\n",
        "from jetbot import Robot\n",
        "import time\n",
        "\n",
        "# Demo code for listening to four hotwords at the same time\n",
        "\n",
        "robot = Robot()\n",
        "interrupted = False\n",
        "\n",
        "def signal_handler(signal, frame):\n",
        "    global interrupted\n",
        "    interrupted = True\n",
        "\n",
        "\n",
        "def interrupt_callback():\n",
        "    global interrupted\n",
        "    return interrupted\n",
        "\n",
        "\n",
        "models = ['wakeup.pmdl', 'stop.pmdl', 'right.pmdl', 'left.pmdl']\n",
        "\n",
        "# initialize\n",
        "update({'new': camera.value})  \n",
        "robot.stop()\n",
        "# You need to set the logic for your command\n",
        "def wakeup():\n",
        "    camera.observe(update, names='value')\n",
        "\n",
        "def stop():\n",
        "    camera.unobserve(update, names='value')\n",
        "    update({'new': camera.value}\n",
        "    robot.stop()\n",
        "\n",
        "def left():\n",
        "    # Turn left for 1 second\n",
        "    robot.left(0.3)\n",
        "    time.sleep(1)\n",
        "    robot.stop()\n",
        "\n",
        "    # Start autonomous driving\n",
        "    camera.observe(update, names='value')\n",
        "\n",
        "def right():\n",
        "    # Turn right for 1 second\n",
        "    robot.right(0.3)\n",
        "    time.sleep(1)\n",
        "    robot.stop()\n",
        "\n",
        "   # Start autonomous driving\n",
        "    camera.observe(update, names='value')\n",
        "# capture SIGINT signal, e.g., Ctrl+\n",
        "\n",
        "signal.signal(signal.SIGINT, signal_handler)\n",
        "sensitivity = [0.5]*len(models)\n",
        "detector = snowboydecoder.HotwordDetector(models, sensitivity=sensitivity)\n",
        "# In order to work properly, you must match the input and your logic sequence in the command.\n",
        "callbacks = [lambda: wakeup(),\n",
        "             lambda: stop(),\n",
        "             lambda: left(),\n",
        "             lambda: right()]\n",
        "\n",
        "print('Listening... Press Ctrl+C to exit')\n",
        "\n",
        "# main loop\n",
        "# make sure you have the same numbers of callbacks and models\n",
        "detector.start(detected_callback=callbacks,\n",
        "               interrupt_check=interrupt_callback,\n",
        "               sleep_time=0.03)\n",
        "\n",
        "detector.terminate()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
