{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "2_Data_collection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG_wFs8QaXRA",
        "colab_type": "text"
      },
      "source": [
        "This Jupyter notebook is to learn an enire process of deep learning applications: i.e., interactive data collection, training, and testing. This material is largely based on the NVIDIA Deep Learning Institute (DLI) course: Getting Started with AI on Jetson Nano. The current implementation is based on PyTorch. You can also find TensorFlow code in the following [link](https://colab.research.google.com/drive/1eym2fLQNkNYl_HCtsLspFdSYD_n_xjDZ). we found that PyTorch requires less memory footprint when compared with TensorFlow. When multiple threads are running simultaneously, TensorFlow becomes so slow--it seems like there is a memory issue.\n",
        "\n",
        " Alternatively, you can install [TensorFlow Lite](https://www.tensorflow.org/lite/guide/get_started), which provides all the tools you need to convert and run TensorFlow models on mobile, embedded, and IoT devices. It support both mobile platforms (Android and iOS) as well as other linux platforms. You should be able to install TensorFlow Lite to Jetson Nano; e.g., please see these articles: lite @ [medium](https://medium.com/@yanweiliu/tflite-on-jetson-nano-c480fdf9ac2) or [stackoverflow](https://stackoverflow.com/questions/60871843/tensorflow-lite-on-nvidia-jetson). There is an interesting article that compares performance of different platforms: [\"Google Coral Edge TPU vs NVIDIA Jetson Nano: A quick deep dive into EdgeAI performance\".](https://blog.usejournal.com/google-coral-edge-tpu-vs-nvidia-jetson-nano-a-quick-deep-dive-into-edgeai-performance-bc7860b8d87a?gi=2b6d1b4feb13)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L_exNKZwh6F",
        "colab_type": "text"
      },
      "source": [
        "#  Data Collection\n",
        "\n",
        "\n",
        "The output of our model is a set of four probabilities:\n",
        "- **p(Left)** - a probability of turning left (spinning counterclockwise)\n",
        "- **p(right)** - a probability of turning right (spinning clockwise)\n",
        "- **p(blocked)** - a probability of the path being blocked\n",
        "- **p(free)** - a probability of no obstacles in front of the robot (so it is safe to move forward)\n",
        "\n",
        "The following method is used collect the data:  \n",
        "\n",
        "First, you'll manually place the robot in scenarios where its \"safety bubble\" is violated, and label these scenarios ``blocked``.  You save a snapshot of what the robot sees along with this label.\n",
        "\n",
        "Second, you'll manually place the robot in scenarios where it's safe to move forward a bit, and label these scenarios ``free``.  Likewise, we save a snapshot along with this label.\n",
        "\n",
        "Thrird, you'll manually place the robot in scenarios where spinning to the left (counterclockwise) would be the optimal move and label these scenarios ``left``. Likewise, you save a snapshot along with this label. Try to vary the angle of the desired rotation - place the robot in scenarios where this angle is larger or smaller.\n",
        "\n",
        "Finally, you'll manually place the robot in scenarios where turning right (clockwise) would be the optimal move and label these scenarios ``right``. Likewise, you save a snapshot along with this label. Try to vary the angle of the desired rotation - place the robot in scenarios where this angle is larger or smaller. \n",
        "\n",
        "Please refer to the following pictures!  \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_eaXS7rIAvO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<img src=\"https://ifh.cc/g/8N3mfC.jpg\" width=200>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPUCAiuCF7Xt",
        "colab_type": "text"
      },
      "source": [
        "Once you have collected 100-200 images for each of four classes, do either one of the following: \n",
        "1. **Upload this data to CoLab for training and download the trained model at your Jetbot**\n",
        "2. Alternatively, train a model locally using JetBot's GPU  \n",
        "\n",
        "But we recommand the first option due to speed reasons!! (it's much faster to train a model at Colab). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h9BqrY2wh6H",
        "colab_type": "text"
      },
      "source": [
        "### Camera\n",
        "\n",
        "First, let's initialize and display\n",
        "\n",
        "> This block sets the size of the images and starts the camera. If your camera is already active in this notebook or in another notebook, first shut down the kernel in the active notebook before running this code cell. Make sure that the correct camera type is selected for execution (CSI or USB). This cell may take several seconds to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Nv0HRUxbwh6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import traitlets\n",
        "import ipywidgets.widgets as widgets\n",
        "from IPython.display import display\n",
        "from jetbot import Camera, bgr8_to_jpeg\n",
        "\n",
        "\n",
        "camera = Camera.instance(width=224, height=224)\n",
        "\n",
        "image = widgets.Image(format='jpeg', width=224, height=224)  #   width and height do not necessarily have to match the camera\n",
        "\n",
        "# Traitlets is a framework that lets Python classes have attributes with type checking, dynamically calculated default values, and ‘on change’ callbacks. \n",
        "# https://traitlets.readthedocs.io/_/downloads/en/stable/pdf/ \n",
        "# dlink: Link the trait of a source object with traits of target objects.\n",
        "# https://traitlets.readthedocs.io/en/stable/utils.html\n",
        "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
        "\n",
        "#display(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg_IkfyEvrrR",
        "colab_type": "text"
      },
      "source": [
        "### Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQkTcVf2wZTH",
        "colab_type": "text"
      },
      "source": [
        "This class is for constructing a dataset, and the image from the JetBot camerais configured as a dataset. \n",
        "\n",
        "\n",
        "\n",
        "*   The ```__getitem__``` method loads your image files and convert them from an jpg file to an array so that you can learn with PyTorch.\n",
        "*   The ```_refresh``` method annotates the path and category at the image you saved.\n",
        "*   The ```save_entry``` method  is a function to save the image received from the camera to a prefined path. Your image files' name determined by ```uuid```, ``uuid`` method to generate\n",
        "a unique identifier.  This unique identifier is generated from information like the current time and the machine address.\n",
        "*   The ```get_count``` method  is a function that stores how many images are stored in your path.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEmZLOpXsN5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "import glob\n",
        "import PIL.Image\n",
        "import subprocess\n",
        "import cv2\n",
        "import os\n",
        "import uuid\n",
        "import subprocess\n",
        "\n",
        "\n",
        "class ImageClassificationDataset(torch.utils.data.Dataset):\n",
        "    \n",
        "    def __init__(self, directory, categories, transform=None):\n",
        "        self.categories = categories\n",
        "        self.directory = directory\n",
        "        self.transform = transform\n",
        "        self._refresh()\n",
        "    \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        ann = self.annotations[idx]\n",
        "        image = cv2.imread(ann['image_path'], cv2.IMREAD_COLOR)\n",
        "        image = PIL.Image.fromarray(image)\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, ann['category_index']\n",
        "    \n",
        "    \n",
        "    def _refresh(self):\n",
        "        self.annotations = []\n",
        "        for category in self.categories:\n",
        "            category_index = self.categories.index(category)\n",
        "            for image_path in glob.glob(os.path.join(self.directory, category, '*.jpg')):\n",
        "                self.annotations += [{\n",
        "                    'image_path': image_path,\n",
        "                    'category_index': category_index,\n",
        "                    'category': category\n",
        "                }]\n",
        "    \n",
        "    def save_entry(self, image, category):\n",
        "        \"\"\"Saves an image in BGR8 format to dataset for category\"\"\"\n",
        "        if category not in self.categories:\n",
        "            raise KeyError('There is no category named %s in this dataset.' % category)\n",
        "            \n",
        "        filename = str(uuid.uuid1()) + '.jpg'\n",
        "        category_directory = os.path.join(self.directory, category)\n",
        "        \n",
        "        if not os.path.exists(category_directory):\n",
        "            subprocess.call(['mkdir', '-p', category_directory])\n",
        "            \n",
        "        image_path = os.path.join(category_directory, filename)\n",
        "        cv2.imwrite(image_path, image)\n",
        "        self._refresh()\n",
        "        return image_path\n",
        "    \n",
        "    def get_count(self, category):\n",
        "        i = 0\n",
        "        for a in self.annotations:\n",
        "            if a['category'] == category:\n",
        "                i += 1\n",
        "        return i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeJAgrgKwoJ5",
        "colab_type": "text"
      },
      "source": [
        "You can organize your dataset by assigning tasks and categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ORrCNPouEnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can chage your task and categories by controlling follwing variables \n",
        "TASK = 'driving'\n",
        "CATEGORIES = ['free', 'left', 'right', 'stop']\n",
        "DATASETS = ['A', 'B']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qVH1h0l-ITa",
        "colab_type": "text"
      },
      "source": [
        "The images saved by JetBot are in jpg format. \n",
        "In order for your robot to learn an image, it is necessary to change the jpg file to a numeric value between [0-225] and convert it into a form  that PyTorch can learn.\n",
        "So when you load your image file, it need to transform properly. (from jpg to torch)\n",
        "\n",
        "\n",
        "Fortunately, PyTorch provides a transform module that can easily transform an image. There are various image transform methods(e.g., color, size), They can be chained together using ```Compose```\n",
        "\n",
        "\n",
        "*   ```ColorJitter``` provide function change the brightness, contrast and saturation of an image. \n",
        "*   ```Resize``` provide your saved image to definded size\n",
        "*   ```ToTensor``` provide send your data CPU to GPU\n",
        "*   ```Normalize``` provide normalize a tensor image with mean and standard deviation\n",
        "\n",
        "You can check detail here: https://pytorch.org/docs/stable/torchvision/transforms.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU49HuTR-Hen",
        "colab_type": "code",
        "outputId": "dc6767cc-4f1e-4955-d9e6-08cfa728f5e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# camera = Camera.instance(width=224, height=224) \n",
        "\n",
        "TRANSFORMS = transforms.Compose([\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2), # ColorJitter(brightness, contrast, saturation, hue)\n",
        "    transforms.Resize((224, 224)), #  Resize((width, height))\n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) #torchvision.transforms.Normalize(mean, std) \n",
        "    # Given mean: (M1,...,Mn) and std: (S1,..,Sn) for n channels, this transform will normalize each channel of the input\n",
        "    # In this case, the number of channel is 3 (RGB)\n",
        "])\n",
        "\n",
        "# After setting your task and data transformation method, create an instance of the dataset using ImageClassificationDataset.\n",
        "datasets = {}\n",
        "for name in DATASETS:\n",
        "    datasets[name] = ImageClassificationDataset(TASK + '_' + name, CATEGORIES, TRANSFORMS)\n",
        "    \n",
        "print(\"{} task with {} categories defined\".format(TASK, CATEGORIES))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "driving task with ['free', 'left', 'right', 'blocked'] categories defined\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9fdMg6X2M402"
      },
      "source": [
        "### Data collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9gSgKlrUFs9M"
      },
      "source": [
        "You'll collect images for your categories with your camera using an iPython widget. This cell sets up the collection mechanism to count your images and produce the user  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyR8OF_jux6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ipywidgets\n",
        "# initialize active dataset\n",
        "dataset = datasets[DATASETS[0]]\n",
        "\n",
        "# unobserve all callbacks from camera in case we are running this cell for second time\n",
        "camera.unobserve_all()\n",
        "\n",
        "# create image preview (camera widget: Provides images observed by the camera in real time)\n",
        "camera_widget = ipywidgets.Image()\n",
        "traitlets.dlink((camera, 'value'), (camera_widget, 'value'), transform=bgr8_to_jpeg)\n",
        "\n",
        "# create widgets  (Widget for organizing datasets)\n",
        "dataset_widget = ipywidgets.Dropdown(options=DATASETS, description='dataset')\n",
        "category_widget = ipywidgets.Dropdown(options=dataset.categories, description='category')\n",
        "count_widget = ipywidgets.IntText(description='count')\n",
        "save_widget = ipywidgets.Button(description='add')\n",
        "\n",
        "# manually update counts at initialization \n",
        "count_widget.value = dataset.get_count(category_widget.value)\n",
        "\n",
        "# sets the active dataset\n",
        "def set_dataset(change):\n",
        "    global dataset\n",
        "    dataset = datasets[change['new']]\n",
        "    count_widget.value = dataset.get_count(category_widget.value)\n",
        "dataset_widget.observe(set_dataset, names='value')\n",
        "\n",
        "# update counts when we select a new category\n",
        "def update_counts(change):\n",
        "    count_widget.value = dataset.get_count(change['new'])\n",
        "category_widget.observe(update_counts, names='value')\n",
        "\n",
        "# save image for category and update counts\n",
        "def save(c):\n",
        "    dataset.save_entry(camera.value, category_widget.value)\n",
        "    count_widget.value = dataset.get_count(category_widget.value)\n",
        "save_widget.on_click(save)\n",
        "\n",
        "data_collection_widget = ipywidgets.VBox([\n",
        "    ipywidgets.HBox([camera_widget]), dataset_widget, category_widget, count_widget, save_widget\n",
        "])\n",
        "\n",
        "\n",
        "display(data_collection_widget)\n",
        "print(\"data_collection_widget created\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsl_umxfwh6T",
        "colab_type": "text"
      },
      "source": [
        "If you refresh the Jupyter file browser on the left, you should now see those directories appearing.  Next, let's create and display some buttons that you'll use to save snapshots for each class label.  You'll also add some text boxes that will display how many images of each category that we've collected so far. This is useful because we want to make sure you collect about the same number of images for each class (``free``, ``left``, ``right`` or ``blocked``).  It also helps to know how many images we've collected overall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54XFE3USwh6a",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Now go ahead and collect some data \n",
        "\n",
        "1. Place the robot in a scenario where it's supposed to turn right and press ``add right``\n",
        "2. Place the robot in a scenario where it's supposed to turn left and press ``add left``\n",
        "3. Place the robot in a scenario where it's free and press ``add free``\n",
        "3. Place the robot in a scenario where it's blocked and press ``add blocked``\n",
        "5. Repeat 1, 2, 3, 4\n",
        "\n",
        "\n",
        "Here are some tips for labeling data\n",
        "\n",
        "1. Try different orientations (e.g. sharp right vs slight right, closer to the cup or further away from it, etc.) \n",
        "2. Try different lighting\n",
        "3. Try different textured floors / objects;  patterned, smooth, glass, etc.\n",
        "\n",
        "Ultimately, the more data we have of scenarios the robot will encounter in the real world, the better our collision avoidance behavior will be.  It's important\n",
        "to get *varied* data (as described by the above tips) and not just a lot of data, but you'll probably need at least 100 images of each class (that's not a science, just a helpful tip here)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui-VRKqlwh6e",
        "colab_type": "text"
      },
      "source": [
        "## Next\n",
        "\n",
        "Once you've collected enough data, you'll need to copy that data to our GPU desktop or cloud machine for training.  First, we can call the following *terminal* command to compress\n",
        "your dataset folder into a single *zip* file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq_THimxwh6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r -q dataset.zip driving_A   # set your dataset folder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUwE9vHXwh6i",
        "colab_type": "text"
      },
      "source": [
        "You should see a file named dataset.zip in the Jupyter Lab file browser.  You should download the zip file using the Jupyter Lab file browser by right clicking and selecting Download.\n",
        "\n",
        "After that, you upload your zip files in google drive. \n",
        "3_Triain_model.ipynb will perform at Colab using a GPU. \n"
      ]
    }
  ]
}
