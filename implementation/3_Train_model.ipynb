{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "3_Train_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7daf04845a434942a1f328375afdfd7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f927fe0b79ba427caa46ecd38341488e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a0e293fceabd42df9c90889db2fad3df",
              "IPY_MODEL_032d2ffd64df4a48a5d2d7be34f23e68"
            ]
          }
        },
        "f927fe0b79ba427caa46ecd38341488e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0e293fceabd42df9c90889db2fad3df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e9cbc5a818fd432fbdf8421a11a86c3b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 244418560,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 244418560,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9f3d8a13eb4948f6bc6ce5df7b4d7ce5"
          }
        },
        "032d2ffd64df4a48a5d2d7be34f23e68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ac852d40042d4e9a92942ca1250341ff",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 233M/233M [00:04&lt;00:00, 54.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f22890d543e540898052956e83e578bb"
          }
        },
        "e9cbc5a818fd432fbdf8421a11a86c3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9f3d8a13eb4948f6bc6ce5df7b4d7ce5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ac852d40042d4e9a92942ca1250341ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f22890d543e540898052956e83e578bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ7sMOtVw2vQ",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Nueral Network - Train Model\n",
        "\n",
        "In this notebook you'll train our image classifier to detect four classes\n",
        "```free```, ```left```, ```right```, and ```blocked```.  For this, we'll use a *PyTorch* due to memory concerns. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1IJmrAnI8aS",
        "colab_type": "text"
      },
      "source": [
        "Once you've connected to Colab, the first thing you need to do is change your runtime to a GPU.\n",
        "\n",
        "\n",
        "Through the menu at the top, click ```Runtime``` => ```Chage run time type``` => set hardware accelerator ```GPU``` => ```Save```\n",
        "\n",
        "<img src='https://ifh.cc/g/Nt2cGh.jpg' width='1200'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nva98tVzw2vR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zu9y8Smw2vY",
        "colab_type": "text"
      },
      "source": [
        "### Upload and extract dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xBgaxKz_sTWv"
      },
      "source": [
        "If you are training a model in Colab, you should follow this sequence:\n",
        "\n",
        "\n",
        "\n",
        "1.   Upload your zip file in Google drive\n",
        "2.   Connect Google dirve from Colab\n",
        "3.   Unzip your zip file (You must enter the path to your home file)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ABnBc4COsSgz",
        "outputId": "f20aa7d4-e5b9-4c8c-9bbf-d7ae389a34c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wto3Z0fh0MVV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Before you start, you should upload the *dataset.zip* file that you created in the *2_Data_collection.ipynb* notebook on the robot.\n",
        "\n",
        "You should then extract this dataset by calling the command below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UI1W8nKw2vY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!unzip path_to_file.zip -d path_to_directory (enter your path)\n",
        "#example\n",
        "!unzip \"/content/drive/My Drive/practice/dataset.zip\" -d \"/content/drive/My Drive/practice\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRrCpQdq0XOT",
        "colab_type": "text"
      },
      "source": [
        "You can easily find the path through the left **files** menu\n",
        "\n",
        "\n",
        "<img src='https://ifh.cc/g/lBOE2d.jpg'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB5rwly9w2vc",
        "colab_type": "text"
      },
      "source": [
        "You should see a folder named *dataset* appear in the file browser."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfNvTnhpw2vd",
        "colab_type": "text"
      },
      "source": [
        "### Create a dataset instance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKrQ2Dqqw2ve",
        "colab_type": "text"
      },
      "source": [
        "Now you use the ImageFolder dataset class available with the torchvision.datasets package and attach transforms from the ``torchvision.transforms`` package to prepare the data for training.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0znU4rrBw2ve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your dataset consists of image files (jpg). Therefore, it must be converted into a form for learning (torch)\n",
        "dataset = datasets.ImageFolder(\n",
        "    '/content/drive/My Drive/practice/dataset', # set your path\n",
        "        transforms.Compose([ # pipline function\n",
        "        transforms.ColorJitter(0.1, 0.1, 0.1, 0.1), # ColorJitter provide function change the brightness, contrast and saturation of an image. \n",
        "        transforms.Resize((224, 224)), # Resize provide your saved image to definded size\n",
        "        transforms.ToTensor(),  # ToTensor provide send your data CPU to GPU\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # # Normalize provide normalize a tensor image with mean and standard deviation\n",
        "    ])\n",
        ")\n",
        "\n",
        "# You can check detail about transformation to follwing link: https://pytorch.org/docs/stable/torchvision/transforms.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1m9AUHUw2vh",
        "colab_type": "text"
      },
      "source": [
        "### Split the dataset into train and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8G4K-kyw2vi",
        "colab_type": "text"
      },
      "source": [
        "Next,  split the dataset into *training* and *test* sets.  The test set will be used to verify the accuracy of the model we train. We use 70 percent of the dataset as a training set and the rest as a test set.\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*-8_kogvwmL1H6ooN1A1tsQ.png' width='500'>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLa8qNsGw2vi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_size = int(len(dataset) * 0.7)\n",
        "test_size = len(dataset) - training_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [training_size, test_size]) \n",
        "#torch.utils.data.random_split(dataset, lengths) \n",
        "#Randomly split a dataset into non-overlapping new datasets of given lengths.\n",
        "#https://pytorch.org/docs/stable/data.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfW0IoJZw2vm",
        "colab_type": "text"
      },
      "source": [
        "### Create data loaders to load data in batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2S68KNVw2vm",
        "colab_type": "text"
      },
      "source": [
        "Create two ``DataLoader`` instances, which provide utilities for shuffling data, producing *batches* of images, and loading the samples in parallel with multiple workers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqaiLVy3w2vn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zYaCWiIw2vq",
        "colab_type": "text"
      },
      "source": [
        "### Define the neural network (CNN)\n",
        "\n",
        "\n",
        "We need to define a Convolutional Neural Network (CNN) model to process the image from the camera. There are two ways to define the model:\n",
        "\n",
        "1. Users create and use model\n",
        "2. Reuse already trained model (transfer learning)\n",
        "\n",
        "\n",
        "You can use either model, but we recommend transfer leraning that you can use efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3CuHIYVpPuu",
        "colab_type": "text"
      },
      "source": [
        "#### 1.User-created model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGpIQK_SqZEJ",
        "colab_type": "text"
      },
      "source": [
        "Design the model as a class. The example below is a layer consisting of two convolutional layers and one fully connected layer. You can add layers for better performance\n",
        "\n",
        "<img src='https://ifh.cc/g/yPYuSj.png'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0EtT8mLptjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # First layer\n",
        "        # ImgIn shape=(?, 224, 224, 3)\n",
        "        #    Conv     -> (?, 224, 224, 32)\n",
        "        #    Pool     -> (?, 112, 112, 32)\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        # Second layer\n",
        "        # ImgIn shape=(?, 112, 112, 32)\n",
        "        #    Conv      ->(?, 112, 112, 64)\n",
        "        #    Pool      ->(?, 56, 56, 64)\n",
        "        self.layer2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        # Fully connected layer with 112x112x64 inputs -> 10 outputs\n",
        "        self.fc = torch.nn.Linear(112 * 112* 64, 4, bias=True)\n",
        "\n",
        "        # initailize fully connected layer's weight\n",
        "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1) \n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq8Se4Drqjhp",
        "colab_type": "text"
      },
      "source": [
        "Define model using the CNN class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n-dDxq2qyHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CNN().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LuuVS_ZoTkj",
        "colab_type": "text"
      },
      "source": [
        "#### 2.Transfer learning\n",
        "The *torchvision* package provides a collection of pre-trained models that we can use.\n",
        "\n",
        "In a process called *transfer learning*, we can repurpose a pre-trained model (trained on millions of images) for a new task that has possibly much less data available.\n",
        "\n",
        "Important features that were learned in the original training of the pre-trained model are re-usable for the new task.  We'll use the alexnet model. But of course, you can use other pre-trained models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6GGNOIcw2vr",
        "colab_type": "code",
        "outputId": "d863ed61-8f39-48e1-a96a-1508395d15d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "7daf04845a434942a1f328375afdfd7f",
            "f927fe0b79ba427caa46ecd38341488e",
            "a0e293fceabd42df9c90889db2fad3df",
            "032d2ffd64df4a48a5d2d7be34f23e68",
            "e9cbc5a818fd432fbdf8421a11a86c3b",
            "9f3d8a13eb4948f6bc6ce5df7b4d7ce5",
            "ac852d40042d4e9a92942ca1250341ff",
            "f22890d543e540898052956e83e578bb"
          ]
        }
      },
      "source": [
        "model = models.alexnet(pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /root/.cache/torch/checkpoints/alexnet-owt-4df8aa71.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7daf04845a434942a1f328375afdfd7f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=244418560.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUWYQW0iw2vv",
        "colab_type": "text"
      },
      "source": [
        "The alexnet model was originally trained for a dataset that had 1000 class labels, but our dataset only has four class labels! You'll replace\n",
        "the final layer with a new, untrained layer that has only four outputs (i.e., free, left, right and blocked.)  \n",
        "\n",
        "<img src='https://www.researchgate.net/profile/Huafeng_Wang4/publication/300412100/figure/fig1/AS:388811231121412@1469711229450/AlexNet-Architecture-To-be-noted-is-copied-2_W640.jpg' >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2GBxfzi-l61",
        "colab_type": "text"
      },
      "source": [
        "Using the summary provided by ```torchsummary```, you can examine the detailed structure of Alexnet. As you can see from the summary, Alexnet consists of a convolutional layer and a classification layer. We fine-tune the number of outputs of the last layer to the number of our classes(4,i.e, free, left, right, block) to create a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbRnpy_4jO9J",
        "colab_type": "code",
        "outputId": "e66123e9-ccba-4965-cf64-8ad3e9c7d2cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(model,  input_size=(3, 224, 224))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 55, 55]          23,296\n",
            "              ReLU-2           [-1, 64, 55, 55]               0\n",
            "         MaxPool2d-3           [-1, 64, 27, 27]               0\n",
            "            Conv2d-4          [-1, 192, 27, 27]         307,392\n",
            "              ReLU-5          [-1, 192, 27, 27]               0\n",
            "         MaxPool2d-6          [-1, 192, 13, 13]               0\n",
            "            Conv2d-7          [-1, 384, 13, 13]         663,936\n",
            "              ReLU-8          [-1, 384, 13, 13]               0\n",
            "            Conv2d-9          [-1, 256, 13, 13]         884,992\n",
            "             ReLU-10          [-1, 256, 13, 13]               0\n",
            "           Conv2d-11          [-1, 256, 13, 13]         590,080\n",
            "             ReLU-12          [-1, 256, 13, 13]               0\n",
            "        MaxPool2d-13            [-1, 256, 6, 6]               0\n",
            "AdaptiveAvgPool2d-14            [-1, 256, 6, 6]               0\n",
            "          Dropout-15                 [-1, 9216]               0\n",
            "           Linear-16                 [-1, 4096]      37,752,832\n",
            "             ReLU-17                 [-1, 4096]               0\n",
            "          Dropout-18                 [-1, 4096]               0\n",
            "           Linear-19                 [-1, 4096]      16,781,312\n",
            "             ReLU-20                 [-1, 4096]               0\n",
            "           Linear-21                 [-1, 1000]       4,097,000\n",
            "================================================================\n",
            "Total params: 61,100,840\n",
            "Trainable params: 61,100,840\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 8.38\n",
            "Params size (MB): 233.08\n",
            "Estimated Total Size (MB): 242.03\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxiidTw2w2vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# As you can see from the above summary, Alexnet consists of a convolutional layer and a classification layer.\n",
        "# Here, model.classifier[6] means the fully connected layer of the last layer (Dropout-15 is the first classification layer)\n",
        "\n",
        "model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 4) # 4 here refers to the number of outputs (class labels)\n",
        "# or you can use model.classifier[-1] instead of model.classifier[6]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2EREvwpw2v0",
        "colab_type": "text"
      },
      "source": [
        "Declaring model training on the GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45lPMDwZw2v1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda')\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0cgd56iAR7I",
        "colab_type": "text"
      },
      "source": [
        "## Train the neural network\n",
        "\n",
        "In deep learning, **backpropagation** is a widely used algorithm in training feedforward neural networks for supervised learning. Generalizations of backpropagation exist for other artificial neural networks (ANNs), and for functions generally – a class of algorithms referred to generically as \"backpropagation.  In fitting a neural network, **backpropagation** computes the gradient of the loss function with respect to the weights of the network for a single input–output example, and does so efficiently, unlike a naive direct computation of the gradient with respect to each weight individually. This efficiency makes it feasible to use gradient methods for training multilayer networks, updating weights to minimize loss; gradient descent, or variants such as stochastic gradient descent, are commonly used. The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the **chain rule**, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this is an example of dynamic programming\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*q1M7LGiDTirwU-4LcFq7_Q.png'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibMpTxRTw2v3",
        "colab_type": "text"
      },
      "source": [
        "### Deep learning with pytorch\n",
        "\n",
        "PyTorch enables the learning process described above. It involves the following steps.\n",
        "\n",
        "1. Send data to the GPU to speed up processing.\n",
        "2. Calculate the loss with forward process.\n",
        "3. Update the weight with backward process.\n",
        "\n",
        "Using the code below you'll train the neural network for 50 epochs, saving the best performing model after each epoch.\n",
        "\n",
        "<img src='https://ifh.cc/g/BOLgNx.png'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "F7XXYAnow2v4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_EPOCHS = 50\n",
        "BEST_MODEL_PATH = '/content/drive/My Drive/practice/best_model.pth' # for BEST_MODEL_PATH, please set your own path (left is an example of the path by TA)\n",
        "best_accuracy = 0.0  # Variable for storing the best performing model in your path\n",
        "\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "    for images, labels in iter(train_loader):\n",
        "        # send data to device then your array data move from CPU to GPU \n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero gradients of parameters\n",
        "        # we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # execute model to get outputs\n",
        "        outputs = model(images)\n",
        "        # compute loss, cross_entropy= −(ylog(p)+(1−y)log(1−p)),\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "        # run backpropogation to accumulate gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # step optimizer to adjust parameters\n",
        "        optimizer.step()\n",
        "    \n",
        "\n",
        "    # After completing one epoch training, you can verify your model through a test set.\n",
        "    test_error_count = 0.0\n",
        "    for images, labels in iter(test_loader):\n",
        "        # send data to device then your array data move from CPU to GPU \n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # execute model to get outputs\n",
        "        outputs = model(images)\n",
        "        # We can get the labels predicted by the trained model through the argmax function\n",
        "        # After that, we calculate the error by comparing it to the actual label\n",
        "        test_error_count += float(torch.sum(torch.abs(labels - outputs.argmax(1))))\n",
        "    \n",
        "    test_accuracy = 1.0 - float(test_error_count) / float(len(test_dataset))\n",
        "    print('%d: %f' % (epoch, test_accuracy))\n",
        "\n",
        "    \n",
        "    # With the following code, the best performing model during training is stored your the path.\n",
        "    if test_accuracy > best_accuracy:\n",
        "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "        best_accuracy = test_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTtnlJifw2v8",
        "colab_type": "text"
      },
      "source": [
        "Once that is finished, you should see a file **best_model.pth** in the your predefined directory.  Select Right click -> Download to download the model to your workstation"
      ]
    }
  ]
}
